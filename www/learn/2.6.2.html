		<p>
		До появления работы Шеннона [6], кодирование символов алфавита при передаче сообщения по каналам связи осуществлялось одинаковым числом бит, получаемым по формуле Хартли. С появлением этой работы начали появляться способы, кодирующие символы разным числом бит в зависимости от вероятности появления их в тексте, т.е. более вероятные символы кодируются короткими кодами, а редко встречающиеся символы - длинными (длиннее среднего). Наиболее&nbsp; известными методами данного класса являются способы кодирования Хаффмана и Шеннона-Фано.</p>
		<p>
			<strong>Алгоритм кодирования (сжатия) информации по Хаффману. </strong>Пусть имеется некоторый алфавит из n символов и длинное сообщение, состоящее из символов данного алфавита. Закодируем сообщение в виде строки битов следующим образом. Каждому символу алфавита присвоим определенную последовательность битов, а затем соединим отдельные коды символов. Пусть, к примеру, алфавит состоит из символов A, B, C, D и пусть им назначены коды:</p>
		<p>
			&nbsp;A - 010, B - 100, C - 000, D - 111. Сообщение ABACCDA кодируется как 010100010000000111010. Такая кодировка неэффективна, так как для каждого символа требуется 3 бита, а для всего сообщения - 21 бит. Назначим каждому символу 2-битовый код: A - 00, B - 01, C - 10, D - 11. Тогда кодировка сообщения будет такова: 00010010101100 (требуется 14 бит). Попробуем найти код, который минимизирует длину закодированного сообщения<strong><em>.</em></strong></p>
		<p>
			&nbsp;Буквы B и D встречаются в сообщении только раз, а буква А - три раза. Выберем код, в котором букве А будет назначена более короткая строка битов, чем буквам B и D. Длина закодированного сообщения станет меньше, так как короткий код встречается чаще, чем длинный. Назначим коды следующим образом: A - 0 (частота 3), B - 110 (частота 1), C - 10 (частота 2), D - 111 (частота 1). Тогда наше сообщение закодируется как 0110010101110 (13 бит). В очень длинных сообщениях экономия существенна. Обычно коды создаются на основе частоты вхождения символов во всем множестве сообщений.</p>
		<p>
			&nbsp;Если используются коды переменной длины, то код одного символа не должен совпадать с началом кода другого символа. Такое условие должно выполняться, если раскодирование происходит слева направо.</p>
		<p>
			&nbsp;Мы просматриваем битовую строку слева направо. Если первый бит равен 0, то это символ А, иначе B,C или D, и проверяем второй бит. Если он равен 0, то это C, иначе B или D, и проверяем третий бит. Если третий бит равен 0, то это символ B, если же он равен 1, то это символ D. После распознавания первого символа процесс повторяется для нахождения второго символа, начиная со следующего бита.</p>
		<p>
			&nbsp;Если известны частоты появления каждого символа в сообщении, то метод реализации оптимальной системы кодирования таков:</p>
		<p>
			&nbsp;Находим два символа, которые появляются наименее часто (B и D в примере). Будем различать их по последнему биту кодов (0 - B, 1 - D). Соединим их в единый символ BD, частота появления которого есть сумма частот появления B и D, то есть 2. Опять выберем два символа с наименьшей частотой (C и BD). Будем различать их по последнему биту кодов: 0 - C, 1 - BD. Объединим их в один символ CBD с частотой 4. Осталось только два символа: A и CBD. Объединим их в один. Различать их будем также по последнему коду: 0 - A, 1 - CBD.</p>
		<p>
			Символ ACBD содержит весь алфавит, и ему в качестве кода присваивается пустая строка битов нулевой длины. Двум символам, составляющим ACBD (A и CBD), присваиваются коды 0 и 1. Символам, составляющим CBD (C и BD), назначаются коды 10 и 11. Символам, составляющим BD (B и D), присваиваются коды 110 и 111. Таким образом, символам, которые встречаются чаще, присваиваются более короткие коды, чем символам, которые встречаются реже.</p>
		<p>
			<img align="left" alt="Подпись:  
Рис. 2.38.Дерево Хаффмана
" height="232" hspace="12" src="file:///C:/Users/5394~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" width="460" />Операция объединения двух символов в один предполагает использование структуры бинарного дерева.</p>
		<p>
			На рис. 2.38 приведено дерево, построенное с использованием предыдущего примера. Каждый узел содержит символ и его частоту. Такие деревья называют деревьями Хаффмана. Рассмотрим алгоритм Хаффмана.</p>
		<p>
			<img height="2" src="file:///C:/Users/5394~1/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png" width="21" />Код Хаффмана - статистический способ сжатия, который дает снижение средней&nbsp; длины кода используемого для представления символов фиксированного алфавита.&nbsp; &nbsp;Код Хаффмана является примером кода, оптимального в случае, когда все вероятности появления символов в сообщении - целые отрицательные степени двойки.</p>
		<p>
			<strong>Алгоритм построения кода Хаффмана:</strong></p>
		<p>
			1. Выписываем в ряд все символы алфавита в порядке возрастания или убывания вероятности их появления в тексте;</p>
		<p>
			2. Последовательно объединяем два символа с наименьшими вероятностями появления в новый составной символ, вероятность появления которого полагается&nbsp; равной сумме вероятностей составляющих его символов; в конце концов мы построим дерево, каждый узел которого имеет суммарную вероятность всех узлов,&nbsp; находящихся ниже него;</p>
		<p>
		3. Прослеживаем путь к каждому листу дерева, помечая направление к каждому узлу (например, направо - 1, налево - 0) и получаем коды Хафмана.</p>
		<p>
			Для заданного распределения частот символов может существовать несколько возможных кодов Хаффмана, - это дает возможность определить каноническое дерево Хаффмана, соответствующее наиболее компактному представлению исходного&nbsp; текста.</p>
		<p>
		Близким по технике построения к коду Хаффмана являются коды Шеннона-Фано,&nbsp;&nbsp; предложенные Шенноном и Фано в 1948-1949 гг. независимо друг от друга. Их построение может быть осуществлено следующим образом:</p>
		<p>
		1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Сортируем все символы в порядке возрастания вероятности появления их в тексте; </p>
		<p>
		2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Последовательно делим множество символов на два подмножества так, чтобы сумма вероятностей появления символов одного подмножества была примерно равна&nbsp; сумме вероятностей появления символов другого. Для левого подмножества каждому символу приписываем &quot;0&quot;, для правого - &quot;1&quot;. Дальнейшие разбиения повторяются до тех пор, пока все подмножества не будут состоять из одного элемента. </p>
		<p>
